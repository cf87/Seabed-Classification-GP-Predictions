{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cf87/Seabed-Classification-GP-Predictions/blob/main/Seabed_Classification_GP_Predictions_PUBLIC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdZ_6vSj8afD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "from os.path import exists\n",
        "# Set our RNG seed for reproducibility.\n",
        "RANDOM_STATE_SEED = 123\n",
        "np.random.seed(RANDOM_STATE_SEED)\n",
        "\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import glob\n",
        "\n",
        "import itertools\n",
        "\n",
        "import sklearn\n",
        "from sklearn import preprocessing\n",
        "from sklearn import utils\n",
        "from sklearn.model_selection import train_test_split,cross_val_score,KFold,RandomizedSearchCV\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from scipy.stats import randint as sp_randint\n",
        "from scipy.stats import uniform, expon\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "from keras.utils.np_utils import to_categorical\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWKfvZga8iS2"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPnV4FzRK-rk"
      },
      "outputs": [],
      "source": [
        "## list of (estimator, param_dist), where param_dist is used  in RandomizedSearchCV\n",
        "classifiers = [\n",
        "    (KNeighborsClassifier(n_jobs=-1), {\n",
        "        'n_neighbors': sp_randint(4, 10),\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'algorithm':['auto', 'ball_tree', 'kd_tree'],\n",
        "        'p': [1,2]\n",
        "    }),\n",
        "    (SVC(random_state=42,max_iter=20), {\n",
        "        'C': np.logspace(-1, 3, 100),\n",
        "        'kernel': ['rbf','linear'], \n",
        "        'class_weight':['balanced', None]\n",
        "    }),\n",
        "    (MLPClassifier(max_iter=20), {\n",
        "        'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
        "        'activation': ['relu','tanh'],\n",
        "        'solver': ['adam','sgd'],\n",
        "        'alpha': expon(scale=.1),\n",
        "        'learning_rate': ['constant','adaptive'],\n",
        "    }),\n",
        "    (RandomForestClassifier(max_depth=5, n_jobs=-1), {\n",
        "        \"max_depth\": sp_randint(2, 20),\n",
        "        \"n_estimators\": sp_randint(2, 50),\n",
        "        'max_features':[None, 'auto', 'sqrt', 'log2'],\n",
        "        'criterion': ['gini', 'entropy']\n",
        "    }),\n",
        "    (LinearDiscriminantAnalysis(), {\n",
        "        'solver': ['svd', 'lsqr'],\n",
        "        'tol': np.logspace(-3, -1, 3)\n",
        "    }),\n",
        "    (LogisticRegression(random_state=0,n_jobs=-1, max_iter=20), {\n",
        "        'C': [.1, 1, 10, 100, 1000],\n",
        "        'multi_class': [\"auto\", \"ovr\",\"multinomial\"],\n",
        "        'solver': ['sag', 'saga', 'newton-cg'],\n",
        "    }),\n",
        "]\n",
        "names = [e.__class__.__name__ for e, g in classifiers]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KraBFM6tm8Tw"
      },
      "outputs": [],
      "source": [
        "from time import time\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report,accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "\n",
        "# Benchmark metrics for each classifier\n",
        "# Adapted from Peter Prettenhofer, et. al,\n",
        "# https://scikit-learn.org/0.19/auto_examples/text/document_classification_20newsgroups.html\n",
        "\n",
        "def benchmark(model):\n",
        "    print(name)\n",
        "    global X_test, y_test\n",
        "    t0 = time()\n",
        "    model.fit(X_train, y_train)\n",
        "    train_time = time() - t0\n",
        "\n",
        "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=1)\n",
        "    train_scores = cross_val_score(model, X_train, y_train, scoring='accuracy', n_jobs=-1)\n",
        "    \n",
        "    t0 = time()\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    test_time = time() - t0\n",
        "    \n",
        "    model_rep = str(metrics.classification_report(y_test, y_test_pred))\n",
        "    cm = metrics.confusion_matrix(y_test, y_test_pred)\n",
        "    model_cm = str(cm)\n",
        "    return score, train_scores.mean(), train_scores.std(), y_test, y_test_pred, train_time, test_time, model_rep, model_cm\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrZzseUUth2a"
      },
      "outputs": [],
      "source": [
        "def load_ws_data(table_structure, noiseLevel=1):\n",
        "    # get prepared data structure\n",
        "    data = table_structure[0, 0]['table']['data']\n",
        "    \n",
        "    # get prepared column names\n",
        "    data_cols = [name[0] for name in table_structure[0, 0]['columns'][0]]\n",
        "    table_dict = {}\n",
        " \n",
        "    \n",
        "    for colidx in range(len(data_cols)):\n",
        "        table_dict[data_cols[colidx]] = [np.squeeze(val) for val in data[0, 0][0, colidx]]\n",
        "        \n",
        "    datatable=pd.DataFrame(table_dict)\n",
        "    data=datatable.loc[datatable.Noise==noiseLevel, ['X']]\n",
        "    X=np.asarray([np.asarray(data)[k][0] for k in range(len(data))])\n",
        "    data=datatable.loc[datatable.Noise==noiseLevel, ['Range']]\n",
        "    ran=np.asarray([np.asarray(data)[k][0] for k in range(len(data))])\n",
        "    data=datatable.loc[datatable.Noise==noiseLevel, ['Sediment']]\n",
        "    sed=np.asarray([np.asarray(data)[k][0] for k in range(len(data))])\n",
        "    return X, ran, sed\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ev30cSQMSJtP"
      },
      "outputs": [],
      "source": [
        "import scipy.io as sio\n",
        "\n",
        "safari_data = sio.loadmat('SafariData_struct.mat')\n",
        "mustar_data = sio.loadmat('mustarData_struct')\n",
        "training_data_100 = sio.loadmat('trainingData_100_struct.mat')\n",
        "training_data = sio.loadmat('trainingData_struct.mat')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoB1NgtbnG9w"
      },
      "outputs": [],
      "source": [
        "# Run ML classification over varying noise level, N_train, data type, and classifier\n",
        "\n",
        "stdlist=[1,2,3,4]\n",
        "for N_train in [100, 500, 1000, 5000, 10000]:\n",
        "  for dataType in [ 'Safari', 'mustar']:\n",
        "    filestr='classType+'_ML_'+dataType+'_N_train_'+str(N_train)+'_'+str(Nr)+'.mat'\n",
        "    \n",
        "    cmsslist=[]\n",
        "    print(dataType)\n",
        "    scores=np.zeros((len(names),len(stdlist)),float)\n",
        "    train_scores=np.zeros((len(names),len(stdlist)),float)\n",
        "    train_stds=np.zeros((len(names),len(stdlist)),float)\n",
        "    ytests=[]\n",
        "    ypreds=[]\n",
        "    cms=[[[] for x in stdlist] for y in names]\n",
        "    print(names)\n",
        "    train_times=np.zeros((len(names),len(stdlist)),float)\n",
        "    test_times=np.zeros((len(names),len(stdlist)),float)\n",
        "\n",
        "    \n",
        "\n",
        "    ds_cnt=-1\n",
        "    y_cnt=-1\n",
        "  \n",
        "    for std in stdlist:\n",
        "      ds_cnt=ds_cnt+1\n",
        "      if dataType=='mustar':\n",
        "        X_test, ran_test, sed_test= load_ws_data(mustar_data['T_struct'], noiseLevel=std)\n",
        "        X_train, ran_train, sed_train= load_ws_data(training_data['T_struct'], noiseLevel=std)\n",
        "        img_width=199\n",
        "      elif dataType=='Safari':\n",
        "        X_test, ran_test, sed_test= load_ws_data(safari_data['T_struct'], noiseLevel=std)\n",
        "        X_train, ran_train, sed_train= load_ws_data(training_data_100['T_struct'], noiseLevel=std)\n",
        "        img_width=100\n",
        "    \n",
        "      \n",
        "      \n",
        "      #Train data\n",
        "      X_train = StandardScaler().fit_transform(X_train)\n",
        "      y1=sed_train\n",
        "      y2=ran_train\n",
        "      y_train=[y1[i].strip()+'_' +str(y2[i]) for i in range(len(y2))];\n",
        "\n",
        "      #Test Data \n",
        "      X_test = StandardScaler().fit_transform(X_test)\n",
        "      yT1=sed_test\n",
        "      yT2=ran_test\n",
        "      y_test=[yT1[i].strip()+'_' +str(yT2[i]) for i in range(len(yT2))];   \n",
        "    \n",
        "      \n",
        "      encoder = LabelEncoder()\n",
        "      encoder.fit(np.unique(y))\n",
        "      yC = encoder.transform(y)\n",
        "      labels=encoder.classes_\n",
        "    \n",
        "      # iterate over classifiers\n",
        "      for est_idx, (name, (estimator, param_grid)) in enumerate(zip(names, classifiers)):\n",
        "        y_cnt=y_cnt+1\n",
        "        \n",
        "        # Perform randomized grid search over possible hyperparameters\n",
        "        model = RandomizedSearchCV(estimator=estimator, param_distributions=param_grid, n_iter=5, verbose=0, n_jobs=-1)\n",
        "            \n",
        "        score, val_score,val_std, y_test, y_test_pred, train_time, test_time, model_rep, model_cm=benchmark(model)\n",
        "        scores[est_idx][ds_cnt]=score\n",
        "        train_scores[est_idx][ds_cnt]=val_score\n",
        "        train_stds[est_idx][ds_cnt]=val_std\n",
        "        train_times[est_idx][ds_cnt]=train_time\n",
        "        test_times[est_idx][ds_cnt]=test_time\n",
        "        cms[est_idx][ds_cnt]=model_cm\n",
        "    \n",
        "    \n",
        "\n",
        "    scorelist=[scores[k,:] for k in range(len(names))]\n",
        "    trainscorelist=[train_scores[k,:] for k in range(len(names))]\n",
        "    trainstdlist=[train_stds[k,:] for k in range(len(names))]\n",
        "    train_timeslist=[train_times[k,:] for k in range(len(names))]\n",
        "    test_timeslist=[test_times[k,:] for k in range(len(names))]\n",
        "    cmsslist=[[cms[k][j] for k in range(len(names))] for j in range(len(stdlist))]\n",
        "    \n",
        "    # initialise data of lists.\n",
        "    data = {'Names':names,\n",
        "            'Scores':scorelist,\n",
        "            'trainScores':trainscorelist,\n",
        "            'trainstds':trainstdlist,\n",
        "            'train_time':train_timeslist, 'test_time':test_timeslist}\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "    df\n",
        "    scipy.io.savemat(classType+'_ML_'+dataType+'_N_train_'+str(N_train)+'_'+str(Nr)+'.mat', {'struct':df.to_dict(\"list\"), 'Confusion': cmsslist, 'y_pred':y_test_pred, 'y_test': y_test})   \n",
        "\n",
        "\n",
        "\n",
        "  "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Seabed-Classification-GP-Predictions-PUBLIC.ipynb",
      "provenance": [],
      "mount_file_id": "1SummGguIV63CVks9iU8zr-S0jKeWka_D",
      "authorship_tag": "ABX9TyODpoLwBCFzHPZ68f2jRtjN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}